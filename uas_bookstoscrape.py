# -*- coding: utf-8 -*-
"""UAS bookstoscrape.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mimcDUf1vn9XvBonfJa1pKwsvBFRY7ep
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, r2_score

# Fungsi Scraping Data
@st.cache
def scrape_books():
    import requests
    from bs4 import BeautifulSoup

    base_url = "https://books.toscrape.com/catalogue/"
    start_url = "https://books.toscrape.com/catalogue/page-1.html"

    books_data = []
    max_books = 100

    while start_url and len(books_data) < max_books:
        response = requests.get(start_url)
        soup = BeautifulSoup(response.text, 'html.parser')

        for book in soup.find_all('article', class_='product_pod'):
            if len(books_data) >= max_books:
                break

            title = book.h3.a['title']
            price = book.find('p', class_='price_color').text[1:].replace('Â', '').strip()
            rating = book.p['class'][1]
            availability = book.find('p', class_='instock availability').text.strip()

            books_data.append({
                'Title': title,
                'Price': float(price.replace('£', '')),
                'Rating': rating,
                'Availability': availability
            })

        next_page = soup.find('li', class_='next')
        if next_page:
            next_url = next_page.a['href']
            start_url = base_url + next_url
        else:
            start_url = None

    return pd.DataFrame(books_data)

# Fungsi untuk Preprocessing
def clean_and_preprocess_data(df):
    rating_mapping = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}
    df['Rating'] = df['Rating'].map(rating_mapping)
    df.dropna(inplace=True)
    return df

# Fungsi Visualisasi
def visualize_data(df):
    st.write("### Distribusi Harga Buku")
    fig, ax = plt.subplots()
    sns.histplot(df['Price'], bins=30, kde=True, ax=ax)
    st.pyplot(fig)

    st.write("### Distribusi Rating Buku")
    fig, ax = plt.subplots()
    sns.countplot(x='Rating', data=df, palette='viridis', ax=ax)
    st.pyplot(fig)

# Fungsi Clustering
def perform_clustering(df):
    st.write("### Clustering Buku Berdasarkan Harga dan Rating")
    kmeans = KMeans(n_clusters=3, random_state=42)
    df['Cluster'] = kmeans.fit_predict(df[['Price', 'Rating']])
    fig, ax = plt.subplots()
    sns.scatterplot(x='Price', y='Rating', hue='Cluster', palette='viridis', data=df, ax=ax)
    st.pyplot(fig)

# Fungsi Regresi
def perform_regression(df):
    X = df[['Rating']]
    y = df['Price']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = LinearRegression()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    st.write("### Hasil Regresi Linear")
    st.write(f"Mean Squared Error: {mse:.2f}")
    st.write(f"R-squared: {r2:.2f}")

    fig, ax = plt.subplots()
    sns.scatterplot(x=y_test, y=y_pred, ax=ax)
    ax.set_xlabel('Actual Price')
    ax.set_ylabel('Predicted Price')
    st.pyplot(fig)

# Main Program
def main():
    st.title("Book Store Analysis with Streamlit")

    # Sidebar Menu
    st.sidebar.header("Menu")
    options = st.sidebar.radio("Pilih Skenario", ["Scrape Data", "Eksplorasi Data", "Clustering", "Regresi"])

    # Scraping Data
    if options == "Scrape Data":
        st.write("## Scraping Data dari Website")
        df = scrape_books()
        st.dataframe(df.head())
        st.write("Data berhasil di-scrape!")

    # Eksplorasi Data
    elif options == "Eksplorasi Data":
        df = scrape_books()
        df = clean_and_preprocess_data(df)
        visualize_data(df)

    # Clustering
    elif options == "Clustering":
        df = scrape_books()
        df = clean_and_preprocess_data(df)
        perform_clustering(df)

    # Regresi
    elif options == "Regresi":
        df = scrape_books()
        df = clean_and_preprocess_data(df)
        perform_regression(df)

if __name__ == "__main__":
    main()